:load /C:/pdi_files/pentahoToSpark/simpleLookups.scala
//Doing Simple Lookup
import com.databricks.spark.xml._
:require /C:/data-integration/drivers/postgresql-42.2.22.jar
val order=spark.read.format("xml").option("rowTag","order").xml("C:/pdi_files/input/orders.xml")
val products=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.products").option("user","postgres").option("password","1").load()
val ordersum=order.groupBy($"man_code" as "manu_code",$"prod_code").count()
val mergedf=ordersum.join(products,products("pro_code").contains(ordersum("prod_code")),"inner")
mergedf.select('man_code,'prod_code,'pro_name,'count as "quantity").filter($"count" > $"pro_stock").coalesce(1).write.mode("overwrite").option("header",true).option("delimiter",";").csv("C:/pdi_files/output/products_to_buy")
val customers=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.customers").option("user","postgres").option("password","1").load()
val del1=order.join(customers,order("idcus") === customers("cus_id"),"inner")
val del2=del1.groupBy($"man_code" as "manu_code",$"prod_code").agg(count("_ordernumber") as "quantity",first("add_street") as "street",first("add_zipcode") as "zipcode",first("city_id") as "cityid",first("cus_id") as "cusid",first("cus_lastname") as "lastname",first("cus_name") as "name",first("_ordernumber") as "ordernumber")
val del3=del2.join(products,products("pro_code").contains(del2("prod_code")),"inner")
val del4=del3.filter($"pro_stock" >= $"quantity")
val cities=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.cities").option("user","postgres").option("password","1").load()
val del5=del4.join(cities,del4("cityid") === cities("city_id"),"inner")
val countries=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.countries").option("user","postgres").option("password","1").load()
val del6=del5.join(countries,del5("cou_id") === countries("cou_id"),"inner").drop("cou_id")
del6.select('name,'lastname,'zipcode,'street,'city_name,'country_name,'ordernumber).sort("country_name").coalesce(1).write.mode("overwrite").option("header",true).option("delimiter",";").csv("C:/pdi_files/output/delivery")
del3.filter($"pro_stock" < $"quantity").select('man_code,'prod_code,'quantity,'street,'zipcode,'cityid,'cusid,'lastname,'name,'ordernumber,'pro_name,'pro_stock).coalesce(1).write.mode("overwrite").option("header",true).option("delimiter",";").csv("C:/pdi_files/output/empty_stock")

:load /C:/pdi_files/pentahoToSpark/complexLookups.scala
//Doing Complex Lookup
import com.databricks.spark.xml._
:require /C:/data-integration/drivers/postgresql-42.2.22.jar
val order=spark.read.format("xml").option("rowTag","order").xml("C:/pdi_files/input/orders.xml")
val ordersum=order.groupBy($"man_code",$"prod_code").agg(count("_ordernumber") as "quantity",concat_ws(",",collect_list("idcus")) as "customers")
val products=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.products").option("user","postgres").option("password","1").load()
val mergedf=ordersum.join(products,products("pro_code").contains(ordersum("prod_code")),"inner").filter($"pro_stock" < $"quantity")
val sugg1=mergedf.select('customers,'quantity as "quantity_param",'pro_theme as "theme_param",'pro_name as "puzz_name")
val sugg2=sugg1.as("a").join(products.as("b"),$"a.theme_param" === $"b.pro_theme").filter($"a.quantity_param" < $"b.pro_stock")
import org.apache.spark.sql.expressions.Window
val theme=Window.partitionBy('pro_theme).orderBy('pro_code)
val ranked=sugg2.withColumn("rank", rank over theme)
ranked.select('customers,'quantity_param,'theme_param,'puzz_name,'man_code,'pro_code,'pro_name).filter($"rank" < 5).show(false)

:load /C:/pdi_files/pentahoToSpark/loadingRegion.scala
//Filling DataWarehouse
:require /C:/data-integration/drivers/postgresql-42.2.22.jar
val cities=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.cities").option("user","postgres").option("password","1").load()
val countries=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.countries").option("user","postgres").option("password","1").load()
val regions1=cities.join(countries,cities("cou_id") === countries("cou_id"),"inner").drop("cou_id")
val regions2=regions1.withColumnRenamed("city_name","city").withColumnRenamed("country_name","country").withColumn("region",lit("N/A")).withColumnRenamed("city_id","id_js").withColumn("lastupdate",lit(current_date()))
import org.apache.spark.sql.expressions.Window
val id=Window.partitionBy('region).orderBy('country,'city)
val regions3=regions2.withColumn("id", rank over id)
val regions4=regions3.union(Seq((0,"N/A","N/A","N/A",null,0)).toDF)
val regions5=regions4.select('id,'city,'country,'region,'id_js,'lastupdate)
regions5.write.format("jdbc").mode("overwrite").option("delete from",true).option("url","jdbc:postgresql://localhost:5432/js_dw").option("driver","org.postgresql.Driver").option("dbtable","public.lk_regions").option("user","postgres").option("password","1").save()

:load /C:/pdi_files/pentahoToSpark/addingRegions.scala
//Adding Regions
:require /C:/data-integration/drivers/postgresql-42.2.22.jar
val cities=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.cities").option("user","postgres").option("password","1").load()
val countries=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.countries").option("user","postgres").option("password","1").load()
val regions=cities.join(countries,cities("cou_id") === countries("cou_id"),"inner").drop("cou_id").withColumn("country_name",trim(col("country_name")))
:require /C:/pdi_files/jars/spark-excel_2.12-0.13.7.jar
import com.crealytics.spark.excel._
val regions0=spark.read.format("com.crealytics.spark.excel").option("header",true).load("C:/pdi_files/input/regions.xls")
val regions1=regions.join(regions0,regions("country_name") === regions0("country"),"left").drop("country")
val regions2=regions1.withColumnRenamed("city_name","city").withColumnRenamed("country_name","country").withColumn("dummy",lit("N/A")).withColumnRenamed("city_id","id_js").withColumn("lastupdate",lit(current_date()))
import org.apache.spark.sql.expressions.Window
val id=Window.partitionBy('dummy).orderBy('country,'city)
val regions3=regions2.withColumn("id", rank over id)
val regions4=regions3.union(Seq((0,"N/A","N/A","N/A","N/A",null,0)).toDF)
val regions5=regions4.select('id,'city,'country,'region,'id_js,'lastupdate)
regions5.write.format("jdbc").mode("overwrite").option("delete from",true).option("url","jdbc:postgresql://localhost:5432/js_dw").option("driver","org.postgresql.Driver").option("dbtable","public.lk_regions").option("user","postgres").option("password","1").save()

:load /C:/pdi_files/pentahoToSpark/loadingManufacturers.scala
//Loading the Manufacturers
:require /C:/data-integration/drivers/postgresql-42.2.22.jar
val manufact=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.manufacturers").option("user","postgres").option("password","1").load()
val manufact1=manufact.withColumnRenamed("man_code","id_js").withColumnRenamed("man_desc","name").withColumn("dummy",lit("N/A")).withColumn("lastupdate",lit(current_date()))
import org.apache.spark.sql.expressions.Window
val id=Window.partitionBy('dummy).orderBy('name)
val manufact2=manufact1.withColumn("id", rank over id)
val manufact3=manufact2.select('id,'name,'id_js,'lastupdate)
manufact3.write.format("jdbc").mode("overwrite").option("delete from",true).option("url","jdbc:postgresql://localhost:5432/js_dw").option("driver","org.postgresql.Driver").option("dbtable","public.lk_manufacturers").option("user","postgres").option("password","1").save()

:load /C:/pdi_files/pentahoToSpark/keepingHistory.scala
//Keeping a History of Changes
:require /C:/data-integration/drivers/postgresql-42.2.22.jar
val products=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.products").option("user","postgres").option("password","1").load()
val products1=products.select('pro_code,'man_code,'pro_name,'pro_theme).filter(col("pro_type").contains("PUZZLE"))
val products2=products1.withColumnRenamed("man_code","id_js_man").withColumnRenamed("pro_name","name").withColumn("dummy",lit("N/A")).withColumn("lastupdate",lit(current_timestamp())).withColumnRenamed("pro_theme","theme").withColumnRenamed("pro_code","id_js_prod").withColumn("start_date",lit("1900-01-01").cast("date")).withColumn("end_date",lit("2199-12-31").cast("date")).withColumn("version",lit(1)).withColumn("current",lit("Y"))
import org.apache.spark.sql.expressions.Window
val id=Window.partitionBy('dummy).orderBy('id_js_prod)
val products3=products2.withColumn("id", rank over id)
val products4=products3.select('id,'name,'theme,'id_js_prod,'id_js_man,'start_date,'end_date,'version,'current,'lastupdate)
val lk_puzzles=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/js_dw").option("driver","org.postgresql.Driver").option("dbtable","public.lk_puzzles").option("user","postgres").option("password","1").load().cache()
lk_puzzles.count()
val lk_puzzles1=lk_puzzles.na.drop()
val lk_puzzles2=lk_puzzles1.union(products4)
val lk_puzzles3=lk_puzzles2.dropDuplicates("theme","id_js_prod")
val lk_puzzles4=lk_puzzles3.groupBy('id_js_prod.as("prodid")).count().filter("count > 1")
val lk_puzzles5=lk_puzzles3.join(lk_puzzles4, lk_puzzles3("id_js_prod") === lk_puzzles4("prodid"),"inner")
val lk_puzzles6=lk_puzzles5.withColumn("current", when(lk_puzzles5("lastupdate") < current_timestamp(), "N").otherwise("Y")).withColumn("version", when(lk_puzzles5("lastupdate") < current_timestamp(), $"version").otherwise($"version"+1)).withColumn("start_date", when(lk_puzzles5("lastupdate") < current_timestamp(), $"start_date").otherwise(current_date())).withColumn("end_date", when(lk_puzzles5("lastupdate") < current_timestamp(), current_date()).otherwise($"end_date"))
val lk_puzzles7=lk_puzzles6.drop("prodid","count")
val lk_puzzles8=products4.union(lk_puzzles7)
val lk_puzzles9=lk_puzzles8.sort('id_js_prod,'lastupdate,desc("version"))
val lk_puzzles10=lk_puzzles9.dropDuplicates("theme","id_js_prod","current")
val lk_puzzles11=lk_puzzles10.drop('id).withColumn("dummy",lit("NA"))
import org.apache.spark.sql.expressions.Window
val id=Window.partitionBy('dummy).orderBy('version,'id_js_prod)
val lk_puzzles12=lk_puzzles11.withColumn("id", rank over id)
val lk_puzzles13=lk_puzzles12.select('id,'name,'theme,'id_js_prod,'id_js_man,'start_date,'end_date,'version,'current,'lastupdate)
val lk_puzzles14=lk_puzzles13.union(Seq((0,"N/A","N/A",0,0,null,null,1,"Y",null)).toDF)
lk_puzzles14.write.format("jdbc").mode("overwrite").option("delete from",true).option("url","jdbc:postgresql://localhost:5432/js_dw").option("driver","org.postgresql.Driver").option("dbtable","public.lk_puzzles").option("user","postgres").option("password","1").save()

val date = scala.io.StdIn.readLine("Put the date in format YYYY/MM/DD or press ENTER for today ")
val date1=Seq(date).toDF("inputdate")
val date2=date1.withColumn("changedate", when(length('inputdate) === 0, current_date()).otherwise(to_date('inputdate, "yyyy/MM/dd")))

:load /C:/pdi_files/pentahoToSpark/keepingHistoryOfRegions.scala
//Keeping History of Regions
:require /C:/data-integration/drivers/postgresql-42.2.22.jar
val cities=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.cities").option("user","postgres").option("password","1").load()
val countries=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/postgres").option("driver","org.postgresql.Driver").option("dbtable","public.countries").option("user","postgres").option("password","1").load()
val regions=cities.join(countries,cities("cou_id") === countries("cou_id"),"inner").drop("cou_id").withColumn("country_name",trim(col("country_name")))
:require /C:/pdi_files/jars/spark-excel_2.12-0.13.7.jar
import com.crealytics.spark.excel._
val regions0=spark.read.format("com.crealytics.spark.excel").option("header",true).load("C:/pdi_files/input/regions.xls")
val regions1=regions.join(regions0,regions("country_name") === regions0("country"),"left").drop("country")
val regions2=regions1.withColumn("version_1",lit(null)).withColumn("country_1",lit(null)).withColumn("start_date",lit("1900-01-01").cast("date")).withColumn("end_date",lit("2199-12-31").cast("date")).withColumn("version",lit(1)).withColumn("dummy",lit("N/A")).withColumnRenamed("country_name","country").withColumn("current",lit(current_timestamp()))
import org.apache.spark.sql.expressions.Window
val id=Window.partitionBy('dummy).orderBy('country,'city_name)
val regions3=regions2.withColumn("id", rank over id)
val regions4=regions3.select('id,'start_date,'end_date,'version,'country_1,'version_1,'country,'region,'current)
val date = scala.io.StdIn.readLine("Put the date in format YYYY/MM/DD or press ENTER for today ")
val date1=Seq(date).toDF("inputdate")
val date2=date1.withColumn("changedate", when(length('inputdate) === 0, current_date()).otherwise(to_date('inputdate, "yyyy/MM/dd")))
val lk_regions_2=spark.read.format("jdbc").option("url","jdbc:postgresql://localhost:5432/js_dw").option("driver","org.postgresql.Driver").option("dbtable","public.lk_regions_2").option("user","postgres").option("password","1").load().cache()
lk_regions_2.count()
val lk_regions_21=lk_regions_2.na.drop(Seq("country"))
val lk_regions_22=lk_regions_21.union(regions4)
val lk_regions_23=lk_regions_22.dropDuplicates("region","country")
val lk_regions_24=lk_regions_23.groupBy('country.as("cntr")).count().filter("count > 1")
val lk_regions_25=lk_regions_24.join(date2)
val lk_regions_26=lk_regions_23.join(lk_regions_25, lk_regions_23("country") === lk_regions_25("cntr"),"inner")
val lk_regions_27=lk_regions_26.withColumn("version", when(lk_regions_26("current") < current_timestamp(), $"version").otherwise($"count")).withColumn("start_date", when(lk_regions_26("current") < current_timestamp(), $"start_date").otherwise(lk_regions_26("changedate"))).withColumn("end_date", when(lk_regions_26("current") < current_timestamp() && lk_regions_26("end_date") === "2199-12-31", lk_regions_26("changedate")).otherwise($"end_date"))
val lk_regions_28=lk_regions_27.drop("cntr","count","inputdate","changedate")
val lk_regions_29=regions4.union(lk_regions_28)
val lk_regions_210=lk_regions_29.sort('country,'current,desc("version"))
val lk_regions_211=lk_regions_210.dropDuplicates("region","country")
val lk_regions_212=lk_regions_211.drop('id).withColumn("dummy",lit("NA"))
import org.apache.spark.sql.expressions.Window
val id=Window.partitionBy('dummy).orderBy('version,'country)
val lk_regions_213=lk_regions_212.withColumn("id", rank over id)
val lk_regions_214=lk_regions_213.select('id,'start_date,'end_date,'version,'country_1,'version_1,'country,'region,'current)
val lk_regions_215=lk_regions_214.union(Seq((0,null,null,1,null,null,null,null,null)).toDF)
lk_regions_215.write.format("jdbc").mode("overwrite").option("delete from",true).option("url","jdbc:postgresql://localhost:5432/js_dw").option("driver","org.postgresql.Driver").option("dbtable","public.lk_regions_2").option("user","postgres").option("password","1").save()

